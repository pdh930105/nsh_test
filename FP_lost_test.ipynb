{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from pyhocon import ConfigFactory\n",
    "from models.resnet import resnet50im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = ConfigFactory.parse_file(\"./resnet50_imagenet.hocon\")\n",
    "getattr(conf, 'data_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = resnet50im(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "f = getattr(conf, 'date', None)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Matrix Adder (M, N) -> M\n",
    "def MatrixAdder(tensor, AdderType=\"FP16\"):\n",
    "    epsilon = 1e-10\n",
    "    if not (len(tensor.shape) == 2 or len(tensor.shape) == 3) :\n",
    "        AssertionError(f\"It only supported 2d Matrix, this tensor shape {tensor.shape}\")\n",
    "\n",
    "    if AdderType==\"FP16\":\n",
    "        mantissa = 10\n",
    "    elif AdderType==\"BF16\":\n",
    "        mantissa = 7\n",
    "    elif AdderType==\"FP32\":\n",
    "        mantissa = 22\n",
    "    else:\n",
    "        AssertionError(\"This Adder only supported FP16|BF16|FP32\")\n",
    "    \n",
    "    temp_tensor =tensor.clone()\n",
    "    zero_mask_counter = []\n",
    "    for i in range(tensor.shape[1] -1):\n",
    "        prev = temp_tensor[:, i]\n",
    "        prec = temp_tensor[:, i+1]\n",
    "        log_prev = torch.log2(torch.abs(prev)+epsilon)\n",
    "        log_prec = torch.log2(torch.abs(prec)+epsilon)\n",
    "        zero_mask = torch.abs(log_prec-log_prev) > mantissa\n",
    "        max_log_tensor = prec.clone()\n",
    "        max_log_tensor[log_prec<log_prev] = prev[log_prec<log_prev] # 두 벡터 중 log2 의 value가 큰 값을 가지고 있는 vector 생성\n",
    "        output = prec+prev # 두 벡터를 더함\n",
    "        output[zero_mask] = max_log_tensor[zero_mask] # zero_mask에 해당하는 부분은 log2 value가 큰 값만 저장\n",
    "        temp_tensor[:, i+1] = output\n",
    "        zero_mask_counter.append(zero_mask.sum())\n",
    "    \n",
    "    \n",
    "    return temp_tensor[:, -1], zero_mask_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This TensorChannelAdder (N, C, H, W) -> (1, C, 1, 1)\n",
    "# It is similar result tensor.sum(axis=0).sum(axis=2).sum(axis=3)\n",
    "# We change tensor.transpose(0, 1).reshape(C, chunk, N*H*W//chunk).sum(axis=2).sum(axis=1)\n",
    "def TensorMeanSim(tensor, chunk=1024,AdderType=\"FP16\"):\n",
    "    epsilon = 1e-10\n",
    "    if not len(tensor.shape) ==4 :\n",
    "        AssertionError(f\"It only supported 4d Matrix, but this tensor shape {tensor.shape}\")\n",
    "\n",
    "    if AdderType==\"FP16\":\n",
    "        mantissa = 10\n",
    "    elif AdderType==\"BF16\":\n",
    "        mantissa = 7\n",
    "    elif AdderType==\"FP32\":\n",
    "        mantissa = 22\n",
    "    elif AdderType==\"test\":\n",
    "        mantissa= 100\n",
    "    else:\n",
    "        AssertionError(\"This Adder only supported FP16|BF16|FP32\")\n",
    "    \n",
    "    temp_tensor =tensor.clone()\n",
    "    zero_mask_counter = []\n",
    "    \n",
    "    n, c, h, w = tensor.shape\n",
    "\n",
    "    \n",
    "    if not n*h*w % chunk == 0: \n",
    "        AssertionError(f\"The n*h*w should always be divisible chunk but result {n*h*w % chunk}\")\n",
    "    #change (c, n*h*w//chunk, chunk) \n",
    "    chunk_tensor = temp_tensor.transpose(1, 0).reshape(c, chunk, n*h*w//chunk)\n",
    "    \n",
    "    # first chunk based Adder (last dim size is equal to n*h*w divided by chunk, so last dim adder is always chunk adder)\n",
    "    # (C, chunk, 0) + (C, chunk, 1) = C*chunk adder\n",
    "    # accumulated that result of adder is final values (chunk_tensor[:, :, -1])\n",
    "    for i in range(chunk_tensor.shape[-1] -1):\n",
    "        prev = chunk_tensor[:, :, i]\n",
    "        prec = chunk_tensor[:, :, i+1]\n",
    "        log_prev = torch.log2(torch.abs(prev)+epsilon)\n",
    "        log_prec = torch.log2(torch.abs(prec)+epsilon)\n",
    "        zero_mask = torch.abs(log_prec-log_prev) > mantissa\n",
    "        max_log_tensor = prec.clone()\n",
    "        max_log_tensor[log_prec<log_prev] = prev[log_prec<log_prev] # 두 벡터 중 log2 의 value가 큰 값을 가지고 있는 vector 생성\n",
    "        output = prec+prev # 두 벡터를 더함\n",
    "        output[zero_mask] = max_log_tensor[zero_mask] # zero_mask에 해당하는 부분은 log2 value가 큰 값만 저장\n",
    "        chunk_tensor[:,:, i+1] = output\n",
    "        zero_mask_counter.append(zero_mask.sum())\n",
    "    \n",
    "    sum_tensor = chunk_tensor[:, :, -1] # C, chunk_size\n",
    "    print(f\"chunk based sum result : {sum(zero_mask_counter)}/{c * chunk * (chunk_tensor.shape[-1]-1)} = {sum(zero_mask_counter) / (c * chunk * (chunk_tensor.shape[-1]-1)) * 100}%\")\n",
    "    \n",
    "    for j in range(chunk_tensor.shape[1]-1):\n",
    "        prev = sum_tensor[:, j]\n",
    "        prec = sum_tensor[:, j+1]        \n",
    "        log_prev = torch.log2(torch.abs(prev)+epsilon)\n",
    "        log_prec = torch.log2(torch.abs(prec)+epsilon)\n",
    "        zero_mask = torch.abs(log_prec-log_prev) > mantissa\n",
    "        max_log_tensor = prec.clone()\n",
    "        max_log_tensor[log_prec<log_prev] = prev[log_prec<log_prev] # 두 벡터 중 log2 의 value가 큰 값을 가지고 있는 vector 생성\n",
    "        output = prec+prev # 두 벡터를 더함\n",
    "        output[zero_mask] = max_log_tensor[zero_mask] # zero_mask에 해당하는 부분은 log2 value가 큰 값만 저장\n",
    "        sum_tensor[:, j+1] = output\n",
    "        zero_mask_counter.append(zero_mask.sum())\n",
    "        \n",
    "    # chunk_tensor[:, :, -1] is same to chunk_tensor.sum(dim=-2), and then finally  \n",
    "\n",
    "    print(f\"final sum result : {sum(zero_mask_counter)}/{c*(chunk-1)+c * chunk * (chunk_tensor.shape[-1]-1)} =\\\n",
    "        {sum(zero_mask_counter)/(c*(chunk-1)+ c * chunk * (chunk_tensor.shape[-1]-1)) * 100}%\")\n",
    "    \n",
    "    return sum_tensor[:, -1]/(n*h*w), sum(zero_mask_counter), (n*h*w-1)*c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This TensorChannelAdder (N, C, H, W) -> (1, C, 1, 1)\n",
    "# It is similar result tensor.sum(axis=0).sum(axis=2).sum(axis=3)\n",
    "# We change tensor.transpose(0, 1).reshape(C, chunk, N*H*W//chunk).sum(axis=2).sum(axis=1)\n",
    "def BatchNormMeanSim(tensor, chunk=1024,AdderType=\"FP16\"):\n",
    "    epsilon = 1e-10\n",
    "    if not len(tensor.shape) ==4 :\n",
    "        AssertionError(f\"It only supported 4d Matrix, but this tensor shape {tensor.shape}\")\n",
    "\n",
    "    if AdderType==\"FP16\":\n",
    "        mantissa = 10\n",
    "    elif AdderType==\"BF16\":\n",
    "        mantissa = 7\n",
    "    elif AdderType==\"FP32\":\n",
    "        mantissa = 22\n",
    "    elif AdderType==\"test\":\n",
    "        mantissa= 100\n",
    "    else:\n",
    "        AssertionError(\"This Adder only supported FP16|BF16|FP32\")\n",
    "    \n",
    "    temp_tensor =tensor.clone()\n",
    "    zero_mask_counter = []\n",
    "    \n",
    "    n, c, h, w = tensor.shape\n",
    "\n",
    "    \n",
    "    if not n*h*w % chunk == 0: \n",
    "        AssertionError(f\"The n*h*w should always be divisible chunk but result {n*h*w % chunk}\")\n",
    "    #change (c, n*h*w//chunk, chunk) \n",
    "    chunk_tensor = temp_tensor.transpose(1, 0).reshape(c, chunk, n*h*w//chunk)\n",
    "    \n",
    "    # first chunk based Adder (last dim size is equal to n*h*w divided by chunk, so last dim adder is always chunk adder)\n",
    "    # (C, chunk, 0) + (C, chunk, 1) = C*chunk adder\n",
    "    # accumulated that result of adder is final values (chunk_tensor[:, :, -1])\n",
    "    for i in range(chunk_tensor.shape[-1] -1):\n",
    "        prev = chunk_tensor[:, :, i]\n",
    "        prec = chunk_tensor[:, :, i+1]\n",
    "        log_prev = torch.log2(torch.abs(prev)+epsilon)\n",
    "        log_prec = torch.log2(torch.abs(prec)+epsilon)\n",
    "        zero_mask = torch.abs(log_prec-log_prev) > mantissa\n",
    "        max_log_tensor = prec.clone()\n",
    "        max_log_tensor[log_prec<log_prev] = prev[log_prec<log_prev] # 두 벡터 중 log2 의 value가 큰 값을 가지고 있는 vector 생성\n",
    "        output = prec+prev # 두 벡터를 더함\n",
    "        output[zero_mask] = max_log_tensor[zero_mask] # zero_mask에 해당하는 부분은 log2 value가 큰 값만 저장\n",
    "        chunk_tensor[:,:, i+1] = output\n",
    "        zero_mask_counter.append(zero_mask.sum())\n",
    "    \n",
    "    sum_tensor = chunk_tensor[:, :, -1] # C, chunk_size\n",
    "    print(f\"chunk based sum result : {sum(zero_mask_counter)}/{c * chunk * (chunk_tensor.shape[-1]-1)} = {sum(zero_mask_counter) / (c * chunk * (chunk_tensor.shape[-1]-1)) * 100}%\")\n",
    "    \n",
    "    for j in range(chunk_tensor.shape[1]-1):\n",
    "        prev = sum_tensor[:, j]\n",
    "        prec = sum_tensor[:, j+1]        \n",
    "        log_prev = torch.log2(torch.abs(prev)+epsilon)\n",
    "        log_prec = torch.log2(torch.abs(prec)+epsilon)\n",
    "        zero_mask = torch.abs(log_prec-log_prev) > mantissa\n",
    "        max_log_tensor = prec.clone()\n",
    "        max_log_tensor[log_prec<log_prev] = prev[log_prec<log_prev] # 두 벡터 중 log2 의 value가 큰 값을 가지고 있는 vector 생성\n",
    "        output = prec+prev # 두 벡터를 더함\n",
    "        output[zero_mask] = max_log_tensor[zero_mask] # zero_mask에 해당하는 부분은 log2 value가 큰 값만 저장\n",
    "        sum_tensor[:, j+1] = output\n",
    "        zero_mask_counter.append(zero_mask.sum())\n",
    "        \n",
    "    # chunk_tensor[:, :, -1] is same to chunk_tensor.sum(dim=-2), and then finally  \n",
    "\n",
    "    print(f\"final sum result : {sum(zero_mask_counter)}/{c*(chunk-1)+c * chunk * (chunk_tensor.shape[-1]-1)} =\\\n",
    "        {sum(zero_mask_counter)/(c*(chunk-1)+ c * chunk * (chunk_tensor.shape[-1]-1)) * 100}%\")\n",
    "    \n",
    "    return sum_tensor[:, -1]/(n*h*w), sum(zero_mask_counter), (n*h*w-1)*c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk based sum result : 661010/12533760 = 5.273836612701416%\n",
      "final sum result : 665735/12582864 =        5.290806770324707%\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(32,48,64,128)\n",
    "a = a.type(torch.bfloat16)\n",
    "\n",
    "m, z, t = TensorAdderSim(a, AdderType=\"BF16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-732.0000,  720.0000,  118.5000,  -76.0000,  860.0000,  832.0000,\n",
       "         142.0000, -260.0000,  205.0000, -648.0000,  -36.5000, -976.0000,\n",
       "        -624.0000, -386.0000, -139.0000,  304.0000, -308.0000, -211.0000,\n",
       "        -167.0000, -426.0000,  195.0000, -131.0000,    6.8125,  -95.5000,\n",
       "         268.0000, -376.0000, -808.0000, 1280.0000,    7.6562,  528.0000,\n",
       "         540.0000,  604.0000,  179.0000, -127.5000,   81.5000,  198.0000,\n",
       "         768.0000, -364.0000, -176.0000,  828.0000,  532.0000, -308.0000,\n",
       "        -588.0000, -342.0000, -139.0000, -336.0000,  402.0000,   41.2500],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.transpose(1,0).reshape(a.shape[1], -1).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-732.0000,  720.0000,  119.0000,  -76.0000,  864.0000,  832.0000,\n",
       "         145.0000, -260.0000,  204.0000, -648.0000,  -37.0000, -976.0000,\n",
       "        -620.0000, -384.0000, -138.0000,  306.0000, -308.0000, -211.0000,\n",
       "        -166.0000, -426.0000,  193.0000, -128.0000,    6.6250,  -96.5000,\n",
       "         268.0000, -376.0000, -812.0000, 1280.0000,    5.9375,  528.0000,\n",
       "         536.0000,  604.0000,  178.0000, -128.0000,   83.0000,  199.0000,\n",
       "         768.0000, -362.0000, -177.0000,  828.0000,  536.0000, -308.0000,\n",
       "        -584.0000, -342.0000, -138.0000, -338.0000,  404.0000,   40.2500],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(dim=3).sum(dim=2).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This TensorChannelAdder (N, C, H, W) -> (1, C, 1, 1)\n",
    "# It is similar result tensor.sum(axis=0).sum(axis=2).sum(axis=3)\n",
    "# We change tensor.transpose(0, 1).reshape(C, chunk, N*H*W//chunk).sum(axis=2).sum(axis=1)\n",
    "def BatchNormStdSim(tensor, mean_tensor, chunk=1024, AdderType=\"FP16\"):\n",
    "    epsilon = 1e-10\n",
    "    if not len(tensor.shape) ==4 :\n",
    "        AssertionError(f\"It only supported 4d Matrix, but this tensor shape {tensor.shape}\")\n",
    "\n",
    "    if AdderType==\"FP16\":\n",
    "        mantissa = 10\n",
    "    elif AdderType==\"BF16\":\n",
    "        mantissa = 7\n",
    "    elif AdderType==\"FP32\":\n",
    "        mantissa = 22\n",
    "    elif AdderType==\"test\":\n",
    "        mantissa= 100\n",
    "    else:\n",
    "        AssertionError(\"This Adder only supported FP16|BF16|FP32\")\n",
    "    \n",
    "    temp_tensor =tensor.clone()\n",
    "    zero_mask_counter = []\n",
    "    \n",
    "    n, c, h, w = tensor.shape\n",
    "\n",
    "    \n",
    "    if not n*h*w % chunk == 0: \n",
    "        AssertionError(f\"The n*h*w should always be divisible chunk but result {n*h*w % chunk}\")\n",
    "    #change (c, n*h*w//chunk, chunk) \n",
    "\n",
    "    if mean_tensor.dim() == 1:\n",
    "        #change 4d tensor\n",
    "        mean_tensor = mean_tensor.reshape(1, -1, 1, 1)\n",
    "    elif mean_tensor.dim() !=4:\n",
    "        AssertionError(\"mean_tensor input only 1d or 4d tensor\")\n",
    "    \n",
    "    if not mean_tensor.shape[1] == c:\n",
    "        AssertionError(\"mean_tensor and tensor is required same shape\")\n",
    "    # first computing (X-mean)**2\n",
    "\n",
    "    mean_tensor = torch.zeros_like(temp_tensor) + mean_tensor # broadcasting and same shape result tensor\n",
    "    log_temp_tensor = torch.log2(torch.abs(temp_tensor) + epsilon)\n",
    "    log_temp_mean = torch.log2(torch.abs(mean_tensor) + epsilon)\n",
    "    zero_mask = torch.abs(log_temp_tensor - log_temp_mean) > mantissa\n",
    "    output = temp_tensor - mean_tensor # X - mean(X)\n",
    "    max_log_tensor = temp_tensor.clone()\n",
    "    max_log_tensor[log_temp_tensor<log_temp_mean] = mean_tensor[log_temp_tensor<log_temp_mean] # get log2 max_value\n",
    "    output[zero_mask]=max_log_tensor[zero_mask]\n",
    "    var = output**2 # (X - mean(X))^2\n",
    "\n",
    "    chunk_tensor = var.transpose(1, 0).reshape(c, chunk, n*h*w//chunk)\n",
    "\n",
    "    \n",
    "    # second chunk based Adder (last dim size is equal to n*h*w divided by chunk, so last dim adder is always chunk adder)\n",
    "    # (C, chunk, 0) + (C, chunk, 1) = C*chunk adder\n",
    "    # accumulated that result of adder is final values (chunk_tensor[:, :, -1])\n",
    "    for i in range(chunk_tensor.shape[-1] -1):\n",
    "        prev = chunk_tensor[:, :, i]\n",
    "        prec = chunk_tensor[:, :, i+1]\n",
    "        log_prev = torch.log2(torch.abs(prev)+epsilon)\n",
    "        log_prec = torch.log2(torch.abs(prec)+epsilon)\n",
    "        zero_mask = torch.abs(log_prec-log_prev) > mantissa\n",
    "        max_log_tensor = prec.clone()\n",
    "        max_log_tensor[log_prec<log_prev] = prev[log_prec<log_prev] # 두 벡터 중 log2 의 value가 큰 값을 가지고 있는 vector 생성\n",
    "        output = prec+prev # 두 벡터를 더함\n",
    "        output[zero_mask] = max_log_tensor[zero_mask] # zero_mask에 해당하는 부분은 log2 value가 큰 값만 저장\n",
    "        chunk_tensor[:,:, i+1] = output\n",
    "        zero_mask_counter.append(zero_mask.sum())\n",
    "    \n",
    "    sum_tensor = chunk_tensor[:, :, -1] # C, chunk_size\n",
    "    print(f\"chunk based sum result : {sum(zero_mask_counter)}/{c * chunk * (chunk_tensor.shape[-1]-1)} = {sum(zero_mask_counter) / (c * chunk * (chunk_tensor.shape[-1]-1)) * 100}%\")\n",
    "    \n",
    "    for j in range(chunk_tensor.shape[1]-1):\n",
    "        prev = sum_tensor[:, j]\n",
    "        prec = sum_tensor[:, j+1]        \n",
    "        log_prev = torch.log2(torch.abs(prev)+epsilon)\n",
    "        log_prec = torch.log2(torch.abs(prec)+epsilon)\n",
    "        zero_mask = torch.abs(log_prec-log_prev) > mantissa\n",
    "        max_log_tensor = prec.clone()\n",
    "        max_log_tensor[log_prec<log_prev] = prev[log_prec<log_prev] # 두 벡터 중 log2 의 value가 큰 값을 가지고 있는 vector 생성\n",
    "        output = prec+prev # 두 벡터를 더함\n",
    "        output[zero_mask] = max_log_tensor[zero_mask] # zero_mask에 해당하는 부분은 log2 value가 큰 값만 저장\n",
    "        sum_tensor[:, j+1] = output\n",
    "        zero_mask_counter.append(zero_mask.sum())\n",
    "        \n",
    "    # chunk_tensor[:, :, -1] is same to chunk_tensor.sum(dim=-2), and then finally  \n",
    "\n",
    "    print(f\"final sum result : {sum(zero_mask_counter)}/{c*(chunk-1)+c * chunk * (chunk_tensor.shape[-1]-1)} =\\\n",
    "        {sum(zero_mask_counter)/(c*(chunk-1)+ c * chunk * (chunk_tensor.shape[-1]-1)) * 100}%\")\n",
    "    \n",
    "    return torch.sqrt(sum_tensor[:, -1]/(n*h*w)), sum(zero_mask_counter), (n*h*w-1)*c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.tensor([1,2,3])\n",
    "c.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11],\n",
      "         [12, 13, 14],\n",
      "         [15, 16, 17],\n",
      "         [18, 19, 20],\n",
      "         [21, 22, 23],\n",
      "         [24, 25, 26],\n",
      "         [27, 28, 29]],\n",
      "\n",
      "        [[30, 31, 32],\n",
      "         [33, 34, 35],\n",
      "         [36, 37, 38],\n",
      "         [39, 40, 41],\n",
      "         [42, 43, 44],\n",
      "         [45, 46, 47],\n",
      "         [48, 49, 50],\n",
      "         [51, 52, 53],\n",
      "         [54, 55, 56],\n",
      "         [57, 58, 59]]])\n",
      "tensor([ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (10) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25348/686293147.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (10) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "a = torch.arange(60).reshape(2,10,3)\n",
    "b = torch.arange(10).reshape(10) *3\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(a-b.repeat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.],\n",
       "         [ 0.,  1.,  2.],\n",
       "         [ 0.,  1.,  2.],\n",
       "         [ 0.,  1.,  2.],\n",
       "         [ 0.,  1.,  2.],\n",
       "         [ 0.,  1.,  2.],\n",
       "         [ 0.,  1.,  2.],\n",
       "         [ 0.,  1.,  2.],\n",
       "         [ 0.,  1.,  2.],\n",
       "         [ 0.,  1.,  2.]],\n",
       "\n",
       "        [[30., 31., 32.],\n",
       "         [30., 31., 32.],\n",
       "         [30., 31., 32.],\n",
       "         [30., 31., 32.],\n",
       "         [30., 31., 32.],\n",
       "         [30., 31., 32.],\n",
       "         [30., 31., 32.],\n",
       "         [30., 31., 32.],\n",
       "         [30., 31., 32.],\n",
       "         [30., 31., 32.]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g=torch.zeros(a.size())\n",
    "g+=b.unsqueeze(0).unsqueeze(2)\n",
    "a-g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  0.,  0.],\n",
       "         [ 3.,  3.,  3.],\n",
       "         [ 6.,  6.,  6.],\n",
       "         [ 9.,  9.,  9.],\n",
       "         [12., 12., 12.],\n",
       "         [15., 15., 15.],\n",
       "         [18., 18., 18.],\n",
       "         [21., 21., 21.],\n",
       "         [24., 24., 24.],\n",
       "         [27., 27., 27.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 3.,  3.,  3.],\n",
       "         [ 6.,  6.,  6.],\n",
       "         [ 9.,  9.,  9.],\n",
       "         [12., 12., 12.],\n",
       "         [15., 15., 15.],\n",
       "         [18., 18., 18.],\n",
       "         [21., 21., 21.],\n",
       "         [24., 24., 24.],\n",
       "         [27., 27., 27.]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0],\n",
      "         [1],\n",
      "         [2],\n",
      "         [3],\n",
      "         [4],\n",
      "         [5],\n",
      "         [6],\n",
      "         [7],\n",
      "         [8],\n",
      "         [9]]])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
